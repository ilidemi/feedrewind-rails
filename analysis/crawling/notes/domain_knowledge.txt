# Crawling
Some blogs require 3 requests per page: http://.../a -> https://.../a -> https://.../a/
Url follows redirects and terminates either at a page with content or in a permanent error
Some parts of url like fragment and tracking query params introduce unnecessary duplication, it's good to discard them with canonicalization
Some redirects require query params (dev.to -> medium.com/m/global-identity -> dev.to/?gi=...), discarding those leads to infinite loop - made redirects look at full url
Some blogs require query params for paging - whitelisted common ones to include in canonical url
Filtering links by original host discards some rss posts (medium + dev.to + towardsdatascience combo), allowing all hosts blows up the volume. Solution is to allow all hosts seen in RSS
Matching redirects on fetch urls introduces more requests if it leads to a known page that would already match a canonical url
I don't have to write smart logic to find RSS link and determine the right one. Feedly seems to only look at marked head links, presenting all, and their own cache
Some blogs can 429 even at 1 request a second after a while


# URL processing
see to_canonical_link_spec


# Engineering
RSS gem fails non-deterministically, parsing at XML level works alright
Parallelization makes reruns faster but Ruby multithreading is single OS thread. Multiprocessing uses all cores but communication is more of a PITA, and a child process can die of OOM and you won't know.
Ruby is unstable in corner cases: segfaults on debugging exceptions, child processes segfault on interrupt, fork doesn't work on windows
ruby-prof sucks for threads and processes


# Boiling the ocean
Caching network requests locally speeds up end to end reruns
Important to support blogging platforms - multiple rss per domain
Some individual blogs are more important than others - look up public page ranks
There is a public crawled dataset with 75M RSS in it
Starting with known 500, then testing on the next 500 from the internet will give an estimate how generalizable the rules are. After that it's like counting the fish in the ocean by sampling.
Rule that works on one website can conflict with another website, but haven't seen that yet.
Make tooling to easier categorize websites manually
Mechanical turk can help with manual efforts
Big e2e dashboard is good
Manual comments in db is good


# Figuring out rules
Manual review is required for the first 500
Force graph layout is useless to see the structure
Even the simplest case like archives page has complications: list sorted either way, duplicate consecutive items in list, two star xpaths, first item highlighted, items shuffled by category, 10 recent + others shuffled
A new blog looks like archives at first (everything on one page), then transitions into another rule
For small WP blogs, paging takes fewer requests that two level archives by month.
RSS can be sorted either way
RSS can have consecutive duplicates
Same autogeneration process can put one node as /p/a and the other as /p[1]/a, so every intermediate node needs an implicit [1]


# User experience
If I've seen a blog a month ago, it published new entries, but rss still overlaps with known ones, I can quickly assemble the new list. Will still need to check for deleted posts in the background.
